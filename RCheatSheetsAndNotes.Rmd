---
title: "LR Cheat Sheets & Notes"
---

[Test Tips], [Code Notes] <br>
Class Notes: [Week 1] , [Week 2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14]<br>
Useful Tidbits: <a href="Notes">Class Notes</a><br>
[Intermediate Stats Notes](IntermediateStatsNotes.html)

### Cheat Sheets

* [R Color Guide](file:///C:/Users/rizen/OneDrive/Documents/BYUI/DataScience Certificate/INT STAT/Statistics-Notebook-master/RColorGuide.html)

* [R Base Graphics Cheat Sheet](http://www.joyce-robbins.com/wp-content/uploads/2016/04/BaseGraphicsCheatsheet.pdf)

* [R Base Commands Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)

* [R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

* [ggplot2 Cheat Sheet](https://rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf)

* [Keyboard Shortcuts](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts)<br/>



### Test Tips

Read output carefully so you understand what it's saying! For quartemile time (qsec) a lower number is  better.<br>
Read carefully so you don't miss info in the questions.<br>
Make sure to check whether the answer is positive or negative.<br>
Make sure you put a zero before the decimal in Canvas.<br>

### Analysis code index
#### Weather Analysis 


#### Theory 1



### Code Notes

#### Summary

Std. Error = estimated standard deviation or standard error or residual standard error
Multiple R-squared (R2) = Multiple R-squared

#### Generic
Ctrl + L to clear console
Getwd()<br>
Glimpse() to see the data and it's types (good for when ? Doesn't work)<br>
In rmd ctr shft c to comment/uncomment<br>
Echo = false means the code won't show just output<br>
Double question marks tells you where<br>
Don't put install in the script cause it'll do it every time<br>
Say no to saving workspace instead save a script with everything.<br>
Shortcut for pipe is ctrl + shift + m<br>

#### Graphics
Notebook add links to rmd files (can also put line number)<br>
P + theme(text=element_text(size24))

#### Problems
Mismatched directories can sometimes cause you problems when you knit.<br> 
Session>Set Working Directory to Source file location<br>
NAs are contagious they will permiate your data, pass na.rm true to remove the nas<br>
Session > restart R because he overwrote his data<br>
Clicking on broom cleans out all the data<br>

#### R Markdown

#### Reminders
Don't do anything to the original data file make sure to assign it to a new name!!!<br>
Filter rows/select columns<br>
Colnames() to see the column names<br>
Tibble() see code for names and first few rows<br>
Easier to do test questions in base R<br>
Filter into a new data set before summarizing<br>

<a id="Notes"></a>

### Class Notes

#### 1, Linear Regression

Three main elements to the mathematical model of regression.

*  The true, unobservable line of the estimated value of the population (regression relation), provides the average Y-value. The expected value of Y is equal to Beta0 + Beta1*Xi $ \underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\ $
*  The dots (regression plus error term), $Y $\epsilon_i$ allows each individual $i$ to deviate from the line. $Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}} + \underbrace{\epsilon_i}_\text{error term} \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)$
*  The estimated line (the line we get from the sample data) $\hat{Y}_i$ is the estimated equation and is interpreted as the estimated average $Y$ for any $X$ $\underbrace{\hat{Y}_i}_{\substack{\text{estimated mean} \\ \text{y-value}}} = \underbrace{b_0 + b_1 X_i}_\text{estimated regression equation}$

$E{Y}$ true mean Y
$Y_i$ the dots (add error term to above), the regression model
$\hat{Y}_i = b_0 + b_1 X_i$, fitted line, lmObject$fitted.values
$b_0$ Estimated intercept, 	b_0 <- mean(Y) - b_1*mean(X)
$b_1$ Estimated slope, b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )
$r_i$ residual-eye, distance of dot from line, lmObject$residuals
$\sigma^2$ variance of $\epsilon_i$

Standard error = estimated standard deviation = the residual standard error in summary
An increase of 1,000 lbs in the weight of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles.

The interpretation of β1 is the amount of increase (or decrease) in the average y-value, denoted E{Y}, per unit change in X. It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”.


##### Class Code

```{r eval=FALSE}
library(tidyverse)
View(airquality)
?airquality

# Basics
library(mosaic) #favstats
favstats(airquality$Temp)
mean(airquality$Temp)
sd(airquality$Temp)

# Histograms
hist(airquality$Temp)

ggplot(airquality, aes(x=Temp)) +
geom_histogram(binwidth=5, fill="skyblue", color="skyblue4") +
labs(title="Maximum daily...", subtitle = "May to September 1973", 
     x="Temperature in Degrees F", y="Number of Days in Temperature Range")

# Boxplot 
boxplot(Temp ~ Month, data=airquality)

ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot()

# Scatterplot
mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)
predict(mylm, data.frame(Wind=19))

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x)

favstats(Temp ~ Month, data=airquality)
ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot(fill="skyblue", color="skyblue3") + 
  labs(title="Maximum daily temperature in degrees Fahrenheit at La Guardia Airport, NY, USA",
       subtitle="May to September 1973",
       y="Temperature in degrees F", x="Month of the Year")


library(tidyverse)

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point(fill="skyblue", pch=21, color="skyblue4") + 
  geom_smooth(method="lm", se=F, formula=y~x, color="skyblue4") + 
  labs(title="Max...", subtitle="May...", 
       y="Temp...", x="Average...")

mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)






set.seed(101) #Allows us to always get the same "random" sample
#Change to a new number to get a new sample

n <- 153 #set the sample size

X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45.

beta0 <- 89 #Our choice for the y-intercept. 

beta1 <- -1.25 #Our choice for the slope. 

sigma <- 1 #Our choice for the std. deviation of the error terms.

epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model

fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data

View(fabData) 

#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.

fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

summary(fab.lm) #Summarize your model. 

plot(y ~ x, data=fabData, ylim=c(20,100)) #Plot the data.

abline(fab.lm) #Add the estimated regression line to your plot.

# Now for something you can't do in real life... but since we created the data...

abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). 

legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n") #Add a legend to your plot specifying which line is which.
```



##### Skills Quiz, Intro to R

```{r}
par(mfrow=c(1,3))
plot(mylm, which = 1:2)
plot(mylm$residuals)
```



#### 2 ri, Sum of Squares, and R-squared  

[Regression Applet](https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html) with sliders

Variance is squared units, sd is in original units
Variance is an average of the sum of squares

*  the values being squared in the numerator are the deviations of each data point from mean
*  Var is large when very spread from mean
*  Var is small when data tightly clustered around mean
*  Var never negative

SSE SSR SSTO R2 r(correlation)


##### Class code



```{r}
# Monday
cars2 <- cars[sample(1:50, 10), ]
View(cars2)

library(tidyverse)

carslm <- lm(dist ~ speed, data=cars2)
summary(carslm)
predict(carslm, data.frame(speed=17.5))

# add line segments
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_segment(aes(x=17.5, xend=17.5, y=45.297, yend=50))


# add squares
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_rect(aes(xmin=17.5, xmax=17.5+.65, ymin=45.297, ymax=50), alpha=0.1) + 
  geom_rect(aes(xmin=speed, xmax=speed+carslm$residuals*.15, ymin=dist, ymax=carslm$fitted.values), alpha=0.1, fill="blue")
```
```{r}
install.packages("plotly")
library(plotly)

x = c(5, 15, 2, 29, 35, 24, 25, 39) 
sum(x)
sum( (1:6)^2 ) 
sum(x^2)
mn <- mean(x)
var(x)
dist <-(x-mn)
dist
sum(dist^2)/7
mean(cars$dist)
sum((1:3)^2)
```

The $i$ in $x_i$ stands for individual
$\bar{x_i}$ is the deviation
Negative areas don't exist. Cannot use the abs value instead of squaring because abs values aren't differentiatable.
$n-1$ (degrees of freedom)--the very last person in has no freedom to select where they sit in the classroom.
If we try to use the same thing twice we get penalized -1.
When add up the squares and divide by n-1 we get the average(ish) of square or mean square divide sum of squares by degrees of freedom
$\hat{Y}$ is the regression line. $\bar{Y}$ is the average flat line. $R^2$ the amount of h variation in Y that we can explain with the regression (our regression has reduced the variation around $\hat{Y}$).

```{r}
# Wednesday
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
cars2 <- cbind(rbind(cars, 
                     cbind(speed=rep(0,50), dist=cars$dist),
                     cbind(speed=cars$speed, dist=cars.lm$fitted.values)),
               frame = rep(c(2,1,3), each=50))
plot_ly(cars2,
        x = ~speed,
        y = ~dist,
        frame = ~frame,
        type = 'scatter',
        mode = 'markers',
        showlegend = F,
        marker=list(color="firebrick")) %>%
  layout(title="Stopping Distance of 1920's Vehicles\n (cars data set)",
         xaxis=list(title="Vehicles Speed in mph (speed)"),
         yaxis=list(title="Stopping Distance in Feet (dist)")) %>%
  add_segments(x = 0, xend = 25, y = mean(cars$dist), yend = mean(cars$dist), line=list(color="gray", dash="dash", width=1), inherit=FALSE, name=TeX("\\bar{Y}"), showlegend=T) %>%
  add_segments(x = 0, xend = 25, y = sum(coef(cars.lm)*c(1,0)), yend = sum(coef(cars.lm)*c(1,25)), line=list(color="darkgray", width=1), inherit=FALSE, name=TeX("\\hat{Y}"), showlegend=T) %>%
  config(mathjax = 'cdn') %>%
  animation_opts(frame=2000, transition=1000, redraw=T)
```

Which graphic works best? Looks like stripchart works for 1 qual and 1 quant (plot, boxplot, stripchart answers)

##### Quizzes

Mobius
```{r}
# wt
mtwt.lm <- lm(mpg~wt, mtcars)
summary(mtwt.lm)

ggplot(mtcars, aes(y=mpg, x=wt)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtwt.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtwt.ssr <- sum( (mtwt.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtwt.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(wtr2 <-mtwt.ssr/mtwt.ssto)


# r is square root of r^2
sqrt(wtr2)

predict(mtwt.lm, data.frame(wt=3*365))

par(mfrow=c(1,3))
plot(mtwt.lm, which = 1:2)
plot(mtwt.lm$residuals)


# cyl
View(mtcars)
mtcyl.lm <- lm(mpg~cyl, mtcars)
summary(mtcyl.lm)
mean(mtcars$mpg)

ggplot(mtcars, aes(y=mpg, x=cyl)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtcyl.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtcyl.ssr <- sum( (mtcyl.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtcyl.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(cylr2 <-mtcyl.ssr/mtcyl.ssto)


# r is square root of r^2
sqrt(cylr2)

predict(mtcyl.lm, data.frame(cyl=3*365))

par(mfrow=c(1,3))
plot(mtcyl.lm, which = 1:2)
plot(mtcyl.lm$residuals)
```

<br>

#### 3

1. State the null and alternative hypotheses.<br>
2. Determine p-value based on test statistic<br>
3 decision regrading Ho<br>
4 State the conclusion (in context). What do you say about Ho<br>
  a. If reject Ho We have sufficient evidence to (Ha in English)<br>
  b. If don't reject Ho- We have insufficient evedience to say that "Ha in English"<br>
Remember Art (Alpha/Type 1 Reject True Ho) and BFF (Beta/Type II Fail to reject False Ho)

                                Ho is True        Ha is True<br>
conclusion: Don't Reject Ho      Correct        Type II error<br>
Reject Ho                       Type I Error        Correct<br>

Probabliity of Type 1 error = alpha (convicting an innocent man of a crime)<br>
Probability of Type II Error = Beta (letting a guilty man go free)<br>
Null hypothesis in jury is innocence so we either reject nul (guilty) or fail to reject null (not guilty), but we don't accept the null (innocence)<br>

##### Class Code
One sample t-test, xbar (stat), Ho: mu=# Ha: mu!=<>#, Req: Sampling distribution of xbar is normal shape, talks about one variable<br>
Matched Pairs (Means of the Difference) Dbar mean(x1-x2) stat, Ho= mua= 0, Sampling distribution of dbar is normal, kniwing who's ingroup 1 tells you something about who's in group 2 like husbands and wives, siblings, same person measured twice, before and after--variables must have same units<br>


 68-95-99.7 rule, with a normal distribution we can assume that within 2 standard deviations above and below the mean lies 95% of the data.


##### Quizzes

These are small sample sizes. You will need to create Q-Q Plots of each sample of data to determine if the data in each group is normal or not. It turns out that making these Q-Q Plots in R is a little cumbersome. Here is the code that does it correctly.<br>

``` {r, eval=FALSE}
library(car)
qqPlot(uptake ~ Type, data = CO2.chilled.250)
```

##### Assessment Quiz

(no notes)

<br>

#### 4


	
##### Class Notes

<br>



Skills Quiz, Wilcoxon Tests (non-parametric)

Assessment Quiz



<br>

####  5


<br>
##### Class Notes

Tip if having to determine patterns look at the interaction plot

How do we find the spread.
The square root of MSE
Here's the summary output where's the SSE
By using teh degrees of freedom and resdiuals std error you can get back to 



```{r s}
library(ggplot2)
cars.lm <- lm(dist~ speed, data=cars)
summary(cars.lm)
confint(cars.lm)

SSE
15.38^2
*48 is the 

#SSTO
# SSTO = SSE + SSR
R^2 = SSR/SSTO = 1- SSE= SSTO
# Have to be able to go forward and backward with regression output most in cheet sheet
```



figure out how to say these

```{r}
#Faithful
plot(waiting ~ eruptions, data=faithful, pch=21, col="darkgray")
faith.lm <- lm(waiting ~ eruptions, data=faithful)
abline(faith.lm, col="darkgray", lwd=2)
abline(v=seq(1.2, 5, 0.5), lty=2, col="gray")
abline(h=seq(50,905, 10), lty=2, col="gray")

ggplot(faithful, aes(x=eruptions, y=waiting)) +
  geom_point(fill="skybule", color="darkgray", pch=21) +
  geom_smooth(method="lm", formula=y~x, se=T, color="darkgray")

View(faithful)
?faithful
# confidence is always narrow middle less on the ends it's for the line

# This code allows us to accurately (or at least with understanding of our inaccuracy) predict the time to the next eruption.
mypreds <- predict(faith.lm, data.frame(eruptions=1.967), interval="prediction")
# for prediction is for the dots
# prediction always wider it accounts for band and dots
# Notice that your prediction interval is about (not quite, but close) to 2 residual standard errors wider on each end than the confidence interval. Discuss with a peer why you think that is the case
#At xbar the lowest variability
myc <- predict(faith.lm, data.frame(eruptions=1.967), interval="confidence")
# difference between xi and xh (xh might be in the data set, it's a new x)
# MSE measures variance of the data around the line, the average


# creates a condfidence band, the true line falls in there
# but the data doesn't fall in there. We believe the average falls in 
# Who collected this, 1.967
# if I put conf on first it'll be hidden by prediction
ggplot(faithful, aes(x=eruptions, y=waiting)) +
  geom_point(fill="skyblue", color="darkgray", pch=21) +
  geom_smooth(method="lm", formula=y~x, se=T, color="darkgray") +
  geom_segment(aes(x=1.967, xend=1.967, y=mypreds[2], yend=mypreds[3], color = "Prediction", lwd=3)) + # se is standard error
  geom_segment(aes(x=1.967, xend=1.967, y=myc[2], yend=myc[3], color = "Confidence", lwd=3)) 

   
```




```{r}
# Wednesday

```


Confidence is for estimating the mean. Prediction is for estimating an individual. Learning when to use which takes practice. Confidence is always for mean average.

Predict Interval says the actual temp or eruption will fall between _

```{r}

```




```{r s}
library(mosaic)
library(tidyverse)
lm.u <- lm(gasbill ~ temp, data=Utilities)
summary(lm.u)
lm.u.sqsq <- lm(sqrt(sqrt(gasbill))) ~ temp, data=Utilities)
summary(lm.u.sqsq)
plot(lm.u.sqsq, which=1)

# bring curve back
b <- coef(lm.u.sqsq)

pred.u <- predict(lm.u, data.frame(temp=30), interval="prediction")
pred.u.sqsq <- predict(lm.u.sqsq, data.frame(temp=30), interval="prediction")^4

pred.u <- predict(lm.u, data.frame(temp=30), interval="prediction")
pred.u.sqsq <- predict(lm.u.sqsq, data.frame(temp=30), interval="prediction")^4


# residualstandard error same units
# first plot which 2 is mirrored
plot(gasbill ~ temp, data=Utilities)
abline(lm.u, col="hotpink")
abline(225.7804 + 26.45, -2.9704, lty=2, col="hotpink")
abline(225.7804 - 26.45, -2.9704, lty=2, col="hotpink")
curve(((b[1] + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2))
curve(((b[1] + 0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2))
curve(((b[1] - 0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2))
abline(h=pred.u, lty=2)
abline(h=pred.u.sqsq, lty=2, col="skyblue")
abline(v=c(30,70) lty=2, col="hotpink")


# because I made it transparent
ggplot(Utilities, aes(x=temp, y=gasbill)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F) +
  stat_function(fun=function(x) (b[1]+b[2]*x)^4) +
  


# we'd go look up the ones that look like they might be a problem and remove the ones we can


# when you don't have linearity and lack constant ? You have nothing
plot(lm.u, which=1)
install.packages("cars")
library(cars)
boxCox(lm.u)



```

ylim statement to extend the graph to show the prediction intervals
change conclusion to have a prediction interval
bring prediction interval for bought car and the interval for sell the car
discuss them in conclusion

##### Quizzes

 $ \underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\ $
 
 $\underbrace{Y_i}_\text{Scale}  = \beta_0 + \beta_1 \underbrace{X_i}_{Length} + \epsilon_i \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)$

```{r}
library(car)
?Davis

lm.davis <- lm(height ~ weight, data=Davis)
summary(Davis)

ggplot(Davis, aes(y=height, x=weight)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Exercising Individuals", xlab="weight", ylab="height")

plot(lm.davis, which=1)

install.packages("alr4")
library(alr4)
View(BGSall)

BG.lm <- lm(HT18 ~ HT2, data=BGSall)
summary(BG.lm)

ggplot(BGSall, aes(y=HT18, x=HT2)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Height from Age 2 to Age 18", xlab="HT2", ylab="HT18")

confint(BG.lm)
plot(BG.lm, which=1:2)

predict(BG.lm, data.frame(HT2=33*2.54), interval="prediction")
(1.4441-2)/0.1901
2*pt(-abs(-2.92425), df=134)




library(alr3)
wblake <- load(file = "C:/Users/rizen/Downloads/wblake.rda")
View(wblake)


wb.lm <- lm(Scale ~ Length, data=wblake)
summary(wb.lm)
ggplot(wblake, aes(y=Scale, x=Length)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Radius of Key Scale increase by Length of Fish", xlab="Length", ylab="Scale")

sqrt.lm <-lm(sqrt(Scale) ~ Length, data=wblake)
summary(sqrt.lm)
b <- coef(sqrt.lm)
ggplot(wblake, aes(y=sqrt(Scale), x=Length)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Radius of Key Scale increase by Length of Fish", xlab="Length", ylab="Scale")


plot(gasbill ~ temp, data=Utilities)
abline(lm.u, col="hotpink")
abline(225.7804 + 26.45, -2.9704, lty=2, col="hotpink")
abline(225.7804 - 26.45, -2.9704, lty=2, col="hotpink")
curve((b[1] + b[2]*x)^4, add=TRUE, col="skyblue", lwd=2)
curve((b[1]+0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
curve((b[1]-0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
abline(h=pred.u, lty=2, col="hotpink")
abline(h=pred.u.sqsq, lty=2, col="skyblue")
abline(v=c(30,70), lty=2)
abline(h=pred.u2, lty=2, col="hotpink")
abline(h=pred.u.sqsq2, lty=2, col="skyblue")


plot(Scale ~ Length, data=wblake)
abline(wb.lm, col="hotpink")
curve(((ob[1]) + ob[2]*x)^2, add=TRUE, col="skyblue", lwd=2)

abline(h=6.19091, lty=2, col="hotpink")
abline(h=2.460408, lty=2, col="skyblue")
abline(v=250, lty=2, col="hotpink")
abline(h=9.848869, lty=2, col="hotpink")
abline(h=3.171672, lty=2, col="skyblue")



ob <- coef(wb.lm)
summary(wb.lm)

curve((b[1]+0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
curve((b[1]-0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
abline(h=pred.u, lty=2, col="hotpink")
abline(h=pred.u.sqsq, lty=2, col="skyblue")
abline(v=c(30,70), lty=2)
abline(h=pred.u2, lty=2, col="hotpink")
abline(h=pred.u.sqsq2, lty=2, col="skyblue")

boxCox(wb.lm)
confint(wb.lm)
plot(wb.lm, which=1:2)
plot(sqrt.lm, which=1:2)

confint(wb.lm)
confint(sqrt.lm)

wb.preds <- predict(wb.lm, data.frame(Length=250), interval="prediction")
sqrt.preds <- predict(sqrt.lm, data.frame(Length=250), interval="prediction")



```














<br>

