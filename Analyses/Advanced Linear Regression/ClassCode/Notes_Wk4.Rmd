---
title: "425_Notes_Wk4"
author: "Dawn Galloway"
date: "1/23/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
n <- 512
storage <- rep(NA, N) # initiating first is way faster, so we'll make a storage container
storage
for (i in 1:N){
  storage[i] <- 2*i
  cat("i =", i, " and 2*i =", 2*i, " was saved in storage.")
}

storage

#Hint 1 Create a sample of data
# the capital N is the reps, the number of times we do something, but our sample size little n will be fixed
n <- 40
Xi <- rep(seq(30, 100, length.out=n/2), each=2) #n must be even.
Yi <- 2.5 + 3*Xi + rnorm(n, 0, 1.2) # B0 + B1 is fixed (deterministic), rnorm is random (stochastic) as sigma is allowed to increase, we see more fuzz around the data/line and the harder it is to know the truth
# the last r norm is the sigma, we're telling the data what law to obey

# could do runif(n, 30, 100) random uniform, the numbers are different each time and the lowest is may be as low or high as the lowest and highest value
# assumption x is fixed and without error

Hint 2 # Run regression and pull out the coefficients, slopes and intercepts have sampling distributions
# want to look at the distribution of   get a sample, get stat, store stat, get a new sample (scatterplot), then throw it back after you've captured the line
mylm <- lm(Yi ~ Xi)
plot(Yi ~ Xi, xlim=c(30,100), ylim=c(-100, 500))
abline(mylm)
coef(mylm)
coef(mylm)[1] #intercept only
coef(mylm)[2] #slope only



Hint 3 # need to Hint 1 and 2 go into a for loop
# Inference in linear regression, the gray bands are the collection of 2000 regression lines (we look at one of the samples of data, but the whole distribution of means)
# We have the most accuracy and least variability we should make our prediction in the middle, we have an advantage because the variability around Ybar is less than it was when we just guessed

hist(storage)
```

MSE is the average box (mean squared). It is an estimate of sigma^2. It is Y direction variability around the line.
When you take the sqrt of a square you get a distance. The average length of distance.
Sigma^2 of B1, how muchc the slope can vary, is equal of the distribution of Y  (It's like the ssto but in the x direction)
Does it make sence that the variability of the slope is the variation of the difference of x and variton of the same for 
sigma^2_bi = sigma^2/ssum of (xi -xhat)^2 true value
s^2_b1 = MSE/sums(of Xi-xhat)^2 the estimated value. The bottom one is the variability of the run

What would happen to those grey lines. If I were to reduce the distance of the dots, the grey lines would increase (because the distance on x is the denominator). We introduce more variability in the slopes . If we have larger sample, the denominator grows so the variability in y decreases. 


```{r}
install.packages("Ecdat")
library(Ecdat)
library(car)
View(Caschool)
mylm <- lm(testscr ~ mealpct, data=Caschool)
plot(testscr ~ mealpct, data=Caschool)
abline(mylm)
summary(mylm)
confint(mylm, level=.95)
plot(mylm, which=1:2)
plot(mylm$residuals, ylab="Residuals")

View(Clothing)
mylm1 <- lm(tsales ~ hourspw, data=Clothing)
mylm2 <- lm(tsales ~ hourspw, data=Clothing2)
plot(tsales ~ hourspw, data=Clothing2)
abline(mylm)
summary(mylm)
confint(mylm, level=.95)
plot(mylm, which=1:2)
plot(mylm$residuals, ylab="Residuals")
pt(-abs(1500), 398)
Clothing2 <- Clothing[-397,]
bc1 <-boxCox(mylm)
bc$
library(MASS)
bc <- boxcox(mylm1)
bc$
#pull the max lambda
lamdba <-bc$x[which.max(bc$y)]
lamdba
boxcox()
BoxCox(mylm, lam)
library(car)
boxCox(mylm)


library(forecast)
BoxCox(mylm)

pt takes a quantile  or t statistic
zscroe times margin of error sd
pt(quantile, df)
qt(percentil, df) for pvalue
pt only gives us the left tail it has to be on the negative side -abs value or put in a negative number and then you don't have to use -abs
# left-tailed (pt(-abs(tvalue), degreeoffreedom)) double it to get both sides
# now you how to get the p-valueu compute it or use the applet from 221
# pt(-abs((3.9324-5)/.4155), 48)*2
# if our estimate is closer to the truth it'l be a bigger pvalue because our estimate is closer to the new hypothesis than the zero hypothesis it's more likely to happen so # the p-value increases

An interesting hypothesis: is the intercept the actual msrp
automatice values only test the null hypothesis
You can calculate both the 
basic calculation my estimate - /sd
difference between the std error the tvalue is the 
t-value is how many std erros the estimate is from H0:B0=0
What is the probability of being  that far away (maybe pvalu)
Do one percent things happen--I got married.
It could have happened but its not very likely
If this impossible thing is occuring my belief system is wrong. THere must be another reason this happens.
How could this happen if there was a god. God didn't let that happen, man chose this it's all on uspop God forbid it, man did it anyway
When something impossible is happening we actual decide we were wrong reject the null and accept an alternate hypothes
round(pt(-1.285, 13)*2,4)
qt(1-0.05/2, 48) If we ignore the two tails we .05 is alpha 1-alpha over 2 that's confidienc 95 if alphas point 1 then 90 for ronfint
significance is on the outside and confidence is on the outside
what people see verses what we have on the inside
t-value is estimate-  over std error
the test statistic (comes from date) and the pvalue have 1to1 rational()
Show in New Window
critical value is upt to a point from left

   qt(1-0.1/2,13)
t* is a test statistic it's testing how far from the value
critical value
onec I know my confidence I can find my t-value
t* a choice my confidence
test statics t- comes from data and my 
critial value is how far

cmy test statics has to get to reject the null, alpha
test statistic to tstar
our critical value is 1.77 at 90% roszihen ig less than 01 is level of significas
tif tscor vales in tales reject null
if tstor in confidence fail to reject the null
probablity is biger than the absolute value of t
tscore is bigger than tstar then you pvalue issmaller than level of signific
who gives alpha is tstar
who gives pvale the test statistic or tvalue

cars.lm <- (dist ~ speed, cars)
summary(cars.lm)
confinnt(cars.lm)
estimate intercept is little b0 which equals Ybar - b1X   if I have soem data and I know where the midddle of the data is with regards to x the ybal xbr is the middle of the data
what it's saying is slopes rise over run x is run, if you knw how far you're running from zero, then how far should you fall. You should fall the slopexbar times aftere going xbar times over you'll be at 0 ybar minus the slope times xbar
its's like an ssto in the x direction 
std error measures for every other sample the variabilty
confint(cars.lm)
gives 2.5 the lower bound and 97.5 upper boiund these two numbers are centered around the estimate 95 percent confident it's between here and here
YOu don't have to do the carculus, 


```

What matters is how spread out the x-values are and the y. That information, that dna, is always found in one sample. WHen the slope varies a lot, the variabilyt of the y intercept changes. If we select data at the ends, there's more variability than at the middle


If you combine x and y, variances are additive, so Var[x+y] = var[x] + var[y]
Tsig^2_xbar = (sigma/sqrt of n)^2  sigma^2_xbar = sigma^2/n
The farther we get from the interecept the more variability we have. (The origin of the line of information we have assume it startss at the origin, 0 space and 0 time). Carbon dating, most people just throw out the date but not the confidence interval.

The MSE[1/n + x_bar^2/sum of difference of xs^2] the first term is variability in the data and the second is the variabilitiy on the Y

We can wrap those std. errors if we put the distance of 2 standard errors around our sample, we know that 95% we'll be correct.
95% confidience the interval contains the truth 95% of the time. We don't know which 5% of the time we're wrong.


